@article{Shitov2014,
  title={The complexity of tropical matrix factorization},
  author={Shitov, Yaroslav},
  journal={Advances in Mathematics},
  volume={254},
  pages={138--156},
  year={2014},
  publisher={Elsevier}
}

@article{Goemans1995,
  title={Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming},
  author={Goemans, Michel X and Williamson, David P},
  journal={Journal of the ACM (JACM)},
  volume={42},
  number={6},
  pages={1115--1145},
  year={1995},
  publisher={ACM New York, NY, USA},
  url={https://dl.acm.org/doi/abs/10.1145/227683.227684}
}


@article{Griewank1992,
  title = {Achieving Logarithmic Growth of Temporal and Spatial Complexity in Reverse Automatic Differentiation},
  author = {Griewank, Andreas},
  year = {1992},
  journal = {Optimization Methods and Software},
  volume = {1},
  number = {1},
  pages = {35--54},
  issn = {10294937},
  doi = {10.1080/10556789208805505},
  abstract = {In its basic form the reverse mode of automatic differentiation yields gradient vectors at a small multiple of the computational work needed to evaluate the underlying scalar function. The practical applicability of this temporal complexity result, due originally to Linnainmaa, seemed to be severely limited by the fact that the memory requirement of the basic implementation is proportional to the run time, T, of the original evaluation program, It is shown here that, by a recursive scheme related to the multilevel differentiation approach of Volin and Ostrovskii, the growth in both temporal and spatial complexity can be limited to a fixed multiple of log(T). Other compromises between the run time and memory requirement are possible, so that the reverse mode becomes applicable to computational problems of virtually any size. {\copyright} 1992, Taylor \& Francis Group, LLC. All rights reserved.},
  keywords = {Adjoint,Checkpointing,Complexity,Gradient,Recursion},
  file = {/Users/liujinguo/Zotero/storage/9ALU8UD4/Griewank_1992_Achieving logarithmic growth of temporal and spatial complexity in reverse.pdf}
}

@book{Griewank2008,
  title = {Evaluating {{Derivatives}}},
  author = {Griewank, Andreas and Walther, Andrea},
  year = {2008},
  journal = {Evaluating Derivatives},
  doi = {10.1137/1.9780898717761},
  abstract = {Algorithmic, or automatic, differentiation (AD) is a growing area of theoretical research and software development concerned with the accurate and efficient evaluation of derivatives for function evaluations given as computer programs. The resulting derivative values are useful for all scientific computations that are based on linear, quadratic, or higher order approximations to nonlinear scalar or vector functions. AD has been applied in particular to optimization, parameter identification, nonlinear equation solving, the numerical integration of differential equations, and combinations of these. Apart from quantifying sensitivities numerically, AD also yields structural dependence information, such as the sparsity pattern and generic rank of Jacobian matrices. The field opens up an exciting opportunity to develop new algorithms that reflect the true cost of accurate derivatives and to use them for improvements in speed and reliability. This second edition has been updated and expanded to cover recent developments in applications and theory, including an elegant NP completeness argument by Uwe Naumann and a brief introduction to scarcity, a generalization of sparsity. There is also added material on checkpointing and iterative differentiation. To improve readability the more detailed analysis of memory and complexity bounds has been relegated to separate, optional chapters.The book consists of three parts: a stand-alone introduction to the fundamentals of AD and its software; a thorough treatment of methods for sparse problems; and final chapters on program-reversal schedules, higher derivatives, nonsmooth problems and iterative processes. Each of the 15 chapters concludes with examples and exercises. Audience: This volume will be valuable to designers of algorithms and software for nonlinear computational problems. Current numerical software users should gain the insight necessary to choose and deploy existing AD software tools to the best advantage.},
  isbn = {978-0-89871-659-7},
  file = {/Users/liujinguo/Zotero/storage/PF7YDDDC/Griewank_Walther_2008_Evaluating Derivatives.pdf}
}

@article{Giles2008,
  title = {Collected Matrix Derivative Results for Forward and Reverse Mode {{AD}}},
  author = {Giles, M B},
  year = {2008},
  journal = {Lecture Notes in Computational Science and Engineering},
  volume = {64},
  pages = {34--44},
  abstract = {This paper collects together a number of matrix derivative results which are very useful in forward and reverse mode AD. It highlights in particular the re-markable contribution of a 1948 paper by Dwyer and Macphail which derives the linear and adjoint sensitivities of a matrix product, inverse and determinant, and a number of related results motivated by applications in multivariate analysis in statis-tics.},
  keywords = {forward mode,numerical linear algebra,reverse mode},
  file = {/Users/liujinguo/Zotero/storage/V7RA2N86/Giles_2008_Collected matrix derivative results for forward and reverse mode AD.pdf}
}

@article{Seeger2017,
  title = {Auto-{{Differentiating Linear Algebra}}},
  author = {Seeger, Matthias and Hetzel, Asmus and Dai, Zhenwen and Meissner, Eric and Lawrence, Neil D},
  year = {2017},
  eprint = {1710.08717},
  abstract = {Development systems for deep learning (DL), such as Theano, Torch, TensorFlow, or MXNet, are easy-to-use tools for creating complex neural network models. Since gradient computations are automatically baked in, and execution is mapped to high performance hardware, these models can be trained end-to-end on large amounts of data. However, it is currently not easy to implement many basic machine learning primitives in these systems (such as Gaussian processes, least squares estimation, principal components analysis, Kalman smoothing), mainly because they lack efficient support of linear algebra primitives as differentiable operators. We detail how a number of matrix decompositions (Cholesky, LQ, symmetric eigen) can be implemented as differentiable operators. We have implemented these primitives in MXNet, running on CPU and GPU in single and double precision. We sketch use cases of these new operators, learning Gaussian process and Bayesian linear regression models, where we demonstrate very substantial reductions in implementation complexity and running time compared to previous codes. Our MXNet extension allows end-to-end learning of hybrid models, which combine deep neural networks (DNNs) with Bayesian concepts, with applications in advanced Gaussian process models, scalable Bayesian optimization, and Bayesian active learning.},
  archiveprefix = {arXiv},
  file = {/Users/liujinguo/Zotero/storage/67ACV2Q8/Seeger et al_2017_Auto-Differentiating Linear Algebra.pdf}
}

@misc{Francuz2023,
  title = {Stable and Efficient Differentiation of Tensor Network Algorithms},
  author = {Francuz, Anna and Schuch, Norbert and Vanhecke, Bram},
  year = {2023},
  month = nov,
  number = {arXiv:2311.11894},
  eprint = {2311.11894},
  primaryclass = {cond-mat, physics:physics, physics:quant-ph},
  doi = {10.48550/arXiv.2311.11894},
  urldate = {2024-01-25},
  abstract = {Gradient based optimization methods are the established state-of-the-art paradigm to study strongly entangled quantum systems in two dimensions with Projected Entangled Pair States. However, the key ingredient, the gradient itself, has proven challenging to calculate accurately and reliably in the case of a corner transfer matrix (CTM)-based approach. Automatic differentiation (AD), which is the best known tool for calculating the gradient, still suffers some crucial shortcomings. Some of these are known, like the problem of excessive memory usage and the divergences which may arise when differentiating a singular value decomposition (SVD). Importantly, we also find that there is a fundamental inaccuracy in the currently used backpropagation of SVD that had not been noted before. In this paper, we describe all these problems and provide them with compact and easy to implement solutions. We analyse the impact of these changes and find that the last problem -- the use of the correct gradient -- is by far the dominant one and thus should be considered a crucial patch to any AD application that makes use of an SVD for truncation.},
  archiveprefix = {arXiv},
  keywords = {Condensed Matter - Strongly Correlated Electrons,Physics - Computational Physics,Quantum Physics},
  file = {/Users/liujinguo/Zotero/storage/KLS33LJU/Francuz et al. - 2023 - Stable and efficient differentiation of tensor network algorithms.pdf}
}

@misc{Wan2019,
  title = {Automatic {{Differentiation}} for {{Complex Valued SVD}}},
  author = {Wan, Zhou-Quan and Zhang, Shi-Xin},
  year = {2019},
  month = nov,
  number = {arXiv:1909.02659},
  eprint = {1909.02659},
  primaryclass = {math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1909.02659},
  urldate = {2025-02-01},
  abstract = {In this note, we report the back propagation formula for complex valued singular value decompositions (SVD). This formula is an important ingredient for a complete automatic differentiation(AD) infrastructure in terms of complex numbers, and it is also the key to understand and utilize AD in tensor networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Numerical Analysis,Condensed Matter - Statistical Mechanics,Condensed Matter - Strongly Correlated Electrons,Mathematics - Numerical Analysis,Quantum Physics,Statistics - Machine Learning},
  file = {/Users/liujinguo/Zotero/storage/CWTNXGI5/Wan and Zhang - 2019 - Automatic Differentiation for Complex Valued SVD.pdf}
}

@article{Liao2019,
  title = {Differentiable {{Programming Tensor Networks}}},
  author = {Liao, Hai-jun and Liu, Jin-guo and Wang, Lei and Xiang, Tao},
  year = {2019},
  journal = {Physical Review X},
  volume = {9},
  number = {3},
  pages = {31041},
  publisher = {American Physical Society},
  issn = {2160-3308},
  doi = {10.1103/PhysRevX.9.031041},
  keywords = {computational physics,condensed,doi:10.1103/PhysRevX.9.031041 url:https://doi.org/},
  file = {/Users/liujinguo/Zotero/storage/UUB6BI64/Liao et al_2019_Differentiable Programming Tensor Networks.pdf}
}

@misc{Hubig2019,
  title = {Use and Implementation of Autodifferentiation in Tensor Network Methods with Complex Scalars},
  author = {Hubig, Claudius},
  year = {2019},
  month = sep,
  number = {arXiv:1907.13422},
  eprint = {1907.13422},
  primaryclass = {cond-mat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1907.13422},
  urldate = {2025-02-01},
  abstract = {Following the recent preprints arXiv:1903.09650 and arXiv:1906.04654 we comment on the feasibility of implementation of autodifferentiation in standard tensor network toolkits by briefly walking through the steps to do so. The total implementation effort comes down to fewer than 1000 lines of additional code. We furthermore summarise the current status when the method is applied to cases where the underlying scalars are complex, not real and the final result is a real-valued scalar. It is straightforward to generalise most operations (addition, tensor products and also the QR decomposition) to this case and after the initial submission of these notes, also the adjoint of the complex SVD has been found.},
  archiveprefix = {arXiv},
  keywords = {Condensed Matter - Strongly Correlated Electrons},
  file = {/Users/liujinguo/Zotero/storage/8WF6QFFD/Hubig - 2019 - Use and implementation of autodifferentiation in tensor network methods with complex scalars.pdf}
}

@article{Lorenz1963,
  title={Deterministic nonperiodic flow},
  author={Lorenz, Edward N},
  journal={Journal of atmospheric sciences},
  volume={20},
  number={2},
  pages={130--141},
  year={1963},
  url={https://journals.ametsoc.org/view/journals/atsc/20/2/1520-0469_1963_020_0130_dnf_2_0_co_2.xml}
}

@book{Hirsch2012,
  title={Differential equations, dynamical systems, and an introduction to chaos},
  author={Hirsch, Morris W and Smale, Stephen and Devaney, Robert L},
  year={2012},
  publisher={Academic press}
}

@article{Revels2016,
    title={Forward-Mode Automatic Differentiation in Julia},
    author={Jarrett Revels and Miles Lubin and Theodore Papamarkou},
    year={2016},
    volume={1607.07892},
    journal={arXiv},
    primaryClass={cs.MS}
}


@article{Berenger1994,
  title={A perfectly matched layer for the absorption of electromagnetic waves},
  author={Berenger, Jean-Pierre},
  journal={Journal of computational physics},
  volume={114},
  number={2},
  pages={185--200},
  year={1994},
  publisher={Elsevier}
}

@ARTICLE{Roden2000,
 author = {J. A. Roden and S. D. Gedney},
 title = {Convolution {PML} ({CPML}): {A}n Efficient {FDTD} Implementation
          of the {CFS}-{PML} for Arbitrary Media},
 journal = {Microwave and Optical Technology Letters},
 year = {2000},
 volume = {27},
 number = {5},
 pages = {334-339},
 doi = {10.1002/1098-2760(20001205)27:5 < 334::AID-MOP14>3.0.CO;2-A}}

@article{Martin2008,
  title={An unsplit convolutional perfectly matched layer improved at grazing incidence for seismic wave propagation in poroelastic media},
  author={Martin, Roland and Komatitsch, Dimitri and Ezziani, Abdel{\^a}ziz},
  journal={Geophysics},
  volume={73},
  number={4},
  pages={T51--T61},
  year={2008},
  publisher={Society of Exploration Geophysicists}
}

@article{Zhu2021,
title = {A general approach to seismic inversion with automatic differentiation},
journal = {Computers \& Geosciences},
volume = {151},
pages = {104751},
year = {2021},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2021.104751},
url = {https://www.sciencedirect.com/science/article/pii/S0098300421000595},
author = {Weiqiang Zhu and Kailai Xu and Eric Darve and Gregory C. Beroza},
keywords = {Computational methods, Algorithms, Inverse problems, Parallel and highperformance computing, Software engineering},
abstract = {Imaging Earth structure or seismic sources from seismic data involves minimizing a target misfit function, and is commonly solved through gradient-based optimization. The adjoint-state method has been developed to compute the gradient efficiently; however, its implementation can be time-consuming and difficult. We develop a general seismic inversion framework to calculate gradients using reverse-mode automatic differentiation. The central idea is that adjoint-state methods and reverse-mode automatic differentiation are mathematically equivalent. The mapping between numerical PDE simulation and deep learning allows us to build a seismic inverse modeling library, ADSeismic, based on deep learning frameworks, which supports high performance reverse-mode automatic differentiation on CPUs and GPUs. We demonstrate the performance of ADSeismic on inverse problems related to velocity model estimation, rupture imaging, earthquake location, and source time function retrieval. ADSeismic has the potential to solve a wide variety of inverse modeling applications within a unified framework.}
}

@article{Grote2010,
      title={Efficient PML for the wave equation},
      author={Marcus J. Grote and Imbo Sim},
      year={2010},
      volume={1001.0319},
      journal={arXiv},
      primaryClass={math.NA}
}

@article{Symes2007,
  title={Reverse time migration with optimal checkpointing},
  author={Symes, William W},
  journal={Geophysics},
  volume={72},
  number={5},
  pages={SM213--SM221},
  year={2007},
  publisher={Society of Exploration Geophysicists},
  url={https://library.seg.org/doi/abs/10.1190/1.2742686}
}

@article{vardy1997intractability,
  title={The intractability of computing the minimum distance of a code},
  author={Vardy, Alexander},
  journal={IEEE Transactions on Information Theory},
  volume={43},
  number={6},
  pages={1757--1766},
  year={1997},
  publisher={IEEE}
}

@article{bravyi2024high,
  title={High-threshold and low-overhead fault-tolerant quantum memory},
  author={Bravyi, Sergey and Cross, Andrew W and Gambetta, Jay M and Maslov, Dmitri and Rall, Patrick and Yoder, Theodore J},
  journal={Nature},
  volume={627},
  number={8005},
  pages={778--782},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@article{landahl2011fault,
  title={Fault-tolerant quantum computing with color codes},
  author={Landahl, Andrew J and Anderson, Jonas T and Rice, Patrick R},
  journal={arXiv preprint arXiv:1108.5738},
  year={2011}
}
@article{calderbank1996good,
  title={Good quantum error-correcting codes exist},
  author={Calderbank, A Robert and Shor, Peter W},
  journal={Physical Review A},
  volume={54},
  number={2},
  pages={1098},
  year={1996},
  publisher={APS}
}
@article{steane1996multiple,
  title={Multiple-particle interference and quantum error correction},
  author={Steane, Andrew},
  journal={Proceedings of the Royal Society of London. Series A: Mathematical, Physical and Engineering Sciences},
  volume={452},
  number={1954},
  pages={2551--2577},
  year={1996},
  publisher={The Royal Society London}
}

@book{Moore2011,
  author = {Moore, Cristopher and Mertens, Stephan},
  title = {The Nature of Computation},
  year = {2011},
  isbn = {0199233217},
  publisher = {Oxford University Press, Inc.},
  address = {USA},
  abstract = {Computational complexity is one of the most beautiful fields of modern mathematics, and it is increasingly relevant to other sciences ranging from physics to biology. But this beauty is often buried underneath layers of unnecessary formalism, and exciting recent results like interactive proofs, cryptography, and quantum computing are usually considered too "advanced" to show to the typical student. The aim of this book is to bridge both gaps by explaining the deep ideas of theoretical computer science in a clear and enjoyable fashion, making them accessible to non computer scientists and to computer scientists who finally want to understand what their formalisms are actually telling. This book gives a lucid and playful explanation of the field, starting with P and NP-completeness. The authors explain why the P vs. NP problem is so fundamental, and why it is so hard to resolve. They then lead the reader through the complexity of mazes and games; optimization in theory and practice; randomized algorithms, interactive proofs, and pseudorandomness; Markov chains and phase transitions; and the outer reaches of quantum computing. At every turn, they use a minimum of formalism, providing explanations that are both deep and accessible. The book is intended for graduates and undergraduates, scientists from other areas who have long wanted to understand this subject, and experts who want to fall in love with this field all over again.}
}

@misc{Gao2024,
  title = {Programming Guide for Solving Constraint Satisfaction Problems with Tensor Networks},
  author = {Gao, Xuanzhao and Li, Xiaofeng and Liu, Jinguo},
  year = {2024},
  month = dec,
  number = {arXiv:2501.00227},
  eprint = {2501.00227},
  primaryclass = {physics},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.00227},
  urldate = {2025-03-23},
  abstract = {Constraint satisfaction problems (CSPs) are a class of problems that are ubiquitous in science and engineering. It features a collection of constraints specified over subsets of variables. A CSP can be solved either directly or by reducing it to other problems. This paper introduces the Julia ecosystem for solving and analyzing CSPs, focusing on the programming practices. We introduce some of the important CSPs and show how these problems are reduced to each other. We also show how to transform CSPs into tensor networks, how to optimize the tensor network contraction orders, and how to extract the solution space properties by contracting the tensor networks with generic element types. Examples are given, which include computing the entropy constant, analyzing the overlap gap property, and the reduction between CSPs.},
  archiveprefix = {arXiv},
  keywords = {Physics - Computational Physics},
  file = {/home/leo/snap/zotero-snap/common/Zotero/storage/B3ELP72J/Gao et al. - 2024 - Programming guide for solving constraint satisfaction problems with tensor networks.pdf}
}

@article{Roa2024,
  title = {Probabilistic inference in the era of tensor networks and differential programming},
  author = {Roa-Villescas, Martin and Gao, Xuanzhao and Stuijk, Sander and Corporaal, Henk and Liu, Jin-Guo},
  journal = {Phys. Rev. Res.},
  volume = {6},
  issue = {3},
  pages = {033261},
  numpages = {12},
  year = {2024},
  month = {Sep},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevResearch.6.033261},
  url = {https://link.aps.org/doi/10.1103/PhysRevResearch.6.033261}
}


@article{Liu2023,
  title = {Computing {{Solution Space Properties}} of {{Combinatorial Optimization Problems Via Generic Tensor Networks}}},
  author = {Liu, Jin-Guo and Gao, Xun and Cain, Madelyn and Lukin, Mikhail D. and Wang, Sheng-Tao},
  year = {2023},
  month = jun,
  journal = {SIAM Journal on Scientific Computing},
  volume = {45},
  number = {3},
  pages = {A1239-A1270},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {1064-8275},
  doi = {10.1137/22M1501787},
  urldate = {2024-02-12},
  abstract = {We introduce the concept of mode-k generalized eigenvalues and eigenvectors of a tensor and prove some properties of such eigenpairs. In particular, we derive an upper bound for the number of equivalence classes of generalized tensor eigenpairs using mixed volume. Based on this bound and the structures of tensor eigenvalue problems, we propose two homotopy continuation type algorithms to solve tensor eigenproblems. With proper implementation, these methods can find all equivalence classes of isolated generalized eigenpairs and some generalized eigenpairs contained in the positive dimensional components (if there are any). We also introduce an algorithm that combines a heuristic approach and a Newton homotopy method to extract real generalized eigenpairs from the found complex generalized eigenpairs. A MATLAB software package, TenEig, has been developed to implement these methods. Numerical results are presented to illustrate the effectiveness and efficiency of TenEig for computing complex or real generalized eigenpairs.},
  file = {/Users/liujinguo/Zotero/storage/6FDWXTBK/Liu et al. - 2023 - Computing Solution Space Properties of Combinatori.pdf}
}

@article{Gamarnik2021,
  title = {The Overlap Gap Property : {{A}} Topological Barrier to Optimizing over Random Structures},
  author = {Gamarnik, David},
  year = {2021},
  volume = {118},
  number = {41},
  doi = {10.1073/pnas.2108492118},
  file = {/Users/liujinguo/Zotero/storage/E9BUXGVW/Gamarnik_2021_The overlap gap property.pdf}
}

@article{Kalachev2021,
  title = {Multi-tensor contraction for XEB verification of quantum circuits},
  author = {Kalachev, Gleb and Panteleev, Pavel and Yung, Man-Hong},
  journal = {arXiv:2108.05665},
  year = {2021},
  doi = {10.48550/arXiv.2108.05665},
  url = {10.48550/arXiv.2108.05665},
}

@article{Goto2021,
  title = {High-Performance Combinatorial Optimization Based on Classical Mechanics},
  author = {Goto, Hayato and Endo, Kotaro and Suzuki, Masaru and Sakai, Yoshisato and Kanao, Taro and Hamakawa, Yohei and Hidaka, Ryo and Yamasaki, Masaya and Tatsumura, Kosuke},
  year = {2021},
  month = feb,
  journal = {Science Advances},
  volume = {7},
  number = {6},
  pages = {eabe7953},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/sciadv.abe7953},
  urldate = {2025-03-29},
  abstract = {Quickly obtaining optimal solutions of combinatorial optimization problems has tremendous value but is extremely difficult. Thus, various kinds of machines specially designed for combinatorial optimization have recently been proposed and developed. Toward the realization of higher-performance machines, here, we propose an algorithm based on classical mechanics, which is obtained by modifying a previously proposed algorithm called simulated bifurcation. Our proposed algorithm allows us to achieve not only high speed by parallel computing but also high solution accuracy for problems with up to one million binary variables. Benchmarking shows that our machine based on the algorithm achieves high performance compared to recently developed machines, including a quantum annealer using a superconducting circuit, a coherent Ising machine using a laser, and digital processors based on various algorithms. Thus, high-performance combinatorial optimization is realized by massively parallel implementations of the proposed algorithm based on classical mechanics.},
  file = {/Users/liujinguo/Zotero/storage/SKDSGV8G/Goto et al. - 2021 - High-performance combinatorial optimization based on classical mechanics.pdf}
}

@article{Verlet1967,
  title={Computer" experiments" on classical fluids. I. Thermodynamical properties of Lennard-Jones molecules},
  author={Verlet, Loup},
  journal={Physical review},
  volume={159},
  number={1},
  pages={98},
  year={1967},
  publisher={APS}
}

@misc{Glover2019,
  title = {A {{Tutorial}} on {{Formulating}} and {{Using QUBO Models}}},
  author = {Glover, Fred and Kochenberger, Gary and Du, Yu},
  year = {2019},
  month = nov,
  number = {arXiv:1811.11538},
  eprint = {1811.11538},
  primaryclass = {quant-ph},
  publisher = {arXiv},
  urldate = {2024-04-23},
  abstract = {The Quadratic Unconstrained Binary Optimization (QUBO) model has gained prominence in recent years with the discovery that it unifies a rich variety of combinatorial optimization problems. By its association with the Ising problem in physics, the QUBO model has emerged as an underpinning of the quantum computing area known as quantum annealing and has become a subject of study in neuromorphic computing. Through these connections, QUBO models lie at the heart of experimentation carried out with quantum computers developed by D-Wave Systems and neuromorphic computers developed by IBM. Computational experience is being amassed by both the classical and the quantum computing communities that highlights not only the potential of the QUBO model but also its effectiveness as an alternative to traditional modeling and solution methodologies. This tutorial discloses the basic features of the QUBO model that give it the power and flexibility to encompass the range of applications that have thrust it onto center stage of the optimization field. We show how many different types of constraining relationships arising in practice can be embodied within the "unconstrained" QUBO formulation in a very natural manner using penalty functions, yielding exact model representations in contrast to the approximate representations produced by customary uses of penalty functions. Each step of generating such models is illustrated in detail by simple numerical examples, to highlight the convenience of using QUBO models in numerous settings. We also describe recent innovations for solving QUBO models that offer a fertile avenue for integrating classical and quantum computing and for applying these models in machine learning.},
  archiveprefix = {arXiv},
  keywords = {90C27,Computer Science - Data Structures and Algorithms,Computer Science - Discrete Mathematics,Mathematics - Optimization and Control,Quantum Physics},
  file = {/home/leo/snap/zotero-snap/common/Zotero/storage/3RVWPTXD/Glover et al. - 2019 - A Tutorial on Formulating and Using QUBO Models.pdf}
}

@book{Wong2012,
  title={Simulated annealing for VLSI design},
  author={Wong, DF and Leong, Hon Wai and Liu, HW},
  volume={42},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@article{Geman1984,
  title={Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images},
  author={Geman, Stuart and Geman, Donald},
  journal={IEEE Transactions on pattern analysis and machine intelligence},
  number={6},
  pages={721--741},
  year={1984},
  publisher={IEEE}
}